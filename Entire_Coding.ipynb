{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec860dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (cryptography 42.0.7 (/Users/vallirajasekar/opt/anaconda3/lib/python3.9/site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (cryptography 42.0.7 (/Users/vallirajasekar/opt/anaconda3/lib/python3.9/site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (cryptography 42.0.7 (/Users/vallirajasekar/opt/anaconda3/lib/python3.9/site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (cryptography 42.0.7 (/Users/vallirajasekar/opt/anaconda3/lib/python3.9/site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cryptography 42.0.7 (/Users/vallirajasekar/opt/anaconda3/lib/python3.9/site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (cryptography 42.0.7 (/Users/vallirajasekar/opt/anaconda3/lib/python3.9/site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/vallirajasekar/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /Users/vallirajasekar/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib/lib_lightgbm.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w5/y4zxnb0130zgls3vnryxcytr0000gn/T/ipykernel_16420/2920255965.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightgbm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCVBooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;34m\"\"\"Load LightGBM library.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mlib_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_lib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCFUNCTYPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/ctypes/__init__.py\u001b[0m in \u001b[0;36mLoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dlltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0m__class_getitem__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenericAlias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/vallirajasekar/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /Users/vallirajasekar/opt/anaconda3/lib/python3.9/site-packages/lightgbm/lib/lib_lightgbm.so\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit,RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_percentage_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder ,FunctionTransformer\n",
    "import xgboost as xgb\n",
    "from scipy.special import inv_boxcox\n",
    " \n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import stats\n",
    "import sklearn\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7165d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Workspace and load data\n",
    "ws1 = Workspace.from_config()  # Connect to Azure ML workspace using config file\n",
    "datastore = Datastore.get(ws1, \"azsgcsschecstr03d_datastore\")  # Retrieve datastore by name\n",
    "dataset = Dataset.Tabular.from_parquet_files(path=(datastore, '/S4/PROCESSED_ZONE/SALES_FORECAST/HISTORICAL_SALES_DATA/*'))  # Load parquet files into a dataset\n",
    "df = dataset.to_pandas_dataframe()  # Convert dataset to a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd10c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('COUNTRY_CODE')['SALES_VALUE_USD'].sum().reset_index().sort_values('SALES_VALUE_USD',ascending=False)\n",
    " \n",
    "df_hk = df.query('COUNTRY_CODE==\"TH\"')\n",
    " \n",
    "columns_to_drop = ['BUSINESS_SEGMENT', 'TOTAL_SALES_QTY', 'BUSINESS_LINE_DESC', 'ORD_CURRENCY',\n",
    "                   'BUSINESS_SEGMENT_DESC', 'BUSINESS_MODEL_1_DESC', 'ATC_3_ID', 'ATC_4_ID', 'ATC_2_DESC', 'BRAND_CODE',\n",
    "                    'BILLING_DATE', 'CLIENT_DESC', 'CUST_GRP_SALES_ORDER', 'CHANNEL_1', 'CHANNEL_2', 'CHANNEL_3']\n",
    "df_hk.drop(columns=columns_to_drop, axis=1, inplace=True)  # Drop irrelevant columns\n",
    " \n",
    "df_hk_gp = df_hk.groupby(['COUNTRY_CODE','MATERIAL_GRP4_CODE','ATC_2_ID','MATERIAL_CODE','BILLING_YEAR','BILLING_MONTH']).agg({\n",
    " \n",
    "    'UNIT_PRICE': 'first',\n",
    "   \n",
    "    'LOST_SALES_VALUE_USD': 'sum',\n",
    " \n",
    "    'TOTAL_FOC_SALES_QTY': 'sum',\n",
    " \n",
    "    'SALES_VALUE_USD': 'sum',\n",
    " \n",
    "    'NET_SALES_QTY':'sum'\n",
    " \n",
    "}).reset_index()\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hk_gp['ts_key'] = df_hk_gp['COUNTRY_CODE']+'_'+df_hk_gp['MATERIAL_GRP4_CODE']+'_'+df_hk_gp['ATC_2_ID']+'_'+df_hk_gp['MATERIAL_CODE']\n",
    " \n",
    "df_hk_gp['BILLING_MONTH'] = df_hk_gp['BILLING_MONTH'].str.zfill(2)\n",
    " \n",
    " \n",
    "df_hk_gp['fiscal_month'] = df_hk_gp['BILLING_YEAR']+df_hk_gp['BILLING_MONTH']\n",
    " \n",
    "df_in = df_hk_gp.copy()\n",
    " \n",
    "df_in.drop(['BILLING_MONTH','BILLING_YEAR'],axis=1,inplace=True)\n",
    " \n",
    "nz_df=df_in[df_in['SALES_VALUE_USD']>0]\n",
    "l1=nz_df.groupby('ts_key').filter(lambda x:len(x)>36)['ts_key'].unique()\n",
    "df_new=df_in[df_in.ts_key.isin(l1)]\n",
    " \n",
    "def pad_time_series(df_in:pd.DataFrame) -> pd.DataFrame:\n",
    "    min_month = df_in['fiscal_month'].min()\n",
    "    max_month = df_in['fiscal_month'].max()\n",
    "    fiscal_months = pd.date_range(start=min_month[:6]+\"01\",end=max_month[:6]+\"01\",freq='MS',).strftime('%Y%m').tolist()\n",
    "    unique_ts_key = df_in['ts_key'].unique()\n",
    "    result = pd.DataFrame()\n",
    "    for ts_key in unique_ts_key:\n",
    "        subset = df_in[df_in['ts_key']==ts_key]\n",
    "        COUNTRY_CODE=subset.iloc[0]['COUNTRY_CODE']\n",
    "        MATERIAL_GRP4_CODE = subset.iloc[0]['MATERIAL_GRP4_CODE']\n",
    "        MATERIAL_CODE = subset.iloc[0]['MATERIAL_CODE']\n",
    "        full_index=pd.DataFrame({'fiscal_month':fiscal_months,'MATERIAL_GRP4_CODE':MATERIAL_GRP4_CODE,'MATERIAL_CODE':MATERIAL_CODE,'COUNTRY_CODE':COUNTRY_CODE,\n",
    "        \"ts_key\":ts_key\n",
    "        })\n",
    " \n",
    "        merged_subset = pd.merge(full_index,subset,on=['COUNTRY_CODE','MATERIAL_GRP4_CODE','MATERIAL_CODE','ts_key','fiscal_month'],how='left')\n",
    "        merged_subset['NET_SALES_QTY'].fillna(0,inplace=True)\n",
    "        merged_subset['SALES_VALUE_USD'].fillna(0,inplace=True)\n",
    "        result = pd.concat([result,merged_subset],ignore_index=True)\n",
    "    return result\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "df_in['year'] = df_in['fiscal_month'].str[:4]\n",
    "df_in['month'] = df_in['fiscal_month'].str[4:]\n",
    "df_in['fiscal_month'] = df_in['year'].astype('str')+\"0\"+df_in['month'].astype('str')\n",
    " \n",
    "def fiscal_month_of_year(x):\n",
    "    try:\n",
    "        return int(x)%1000\n",
    "    except:\n",
    "        return None\n",
    " \n",
    "def fiscal_month_from_iso_week(iso_week :int):\n",
    "    year = iso_week//100\n",
    "    week=iso_week%100\n",
    "    month= month_455_from_iso_week(week)\n",
    "    return int(year * 1000 + month)\n",
    " \n",
    "def month_455_from_iso_week(iso_week:int):\n",
    "    iso_week_zero_indexed = iso_week -1\n",
    "    months_elapsed = (iso_week_zero_indexed //13) * 3\n",
    "    week_in_quarter = iso_week_zero_indexed % 13\n",
    "    if week_in_quarter <=3:\n",
    "        return min(months_elapsed + 1,12)\n",
    "    elif week_in_quarter <=7:\n",
    "        return min(months_elapsed + 2, 12)\n",
    "    else:\n",
    "        return min(months_elapsed + 3, 12)\n",
    " \n",
    "def generate_fiscal_calendar( year:int):\n",
    "    iso_week_in_year = date(year, 12, 28).isocalendar()[1]\n",
    "    iso_weeks = [year * 100 + w for w in range ( 1, iso_week_in_year +1 )]\n",
    "    dates = [date_from_calweek(iw) for iw in iso_weeks ]\n",
    "    fiscal_months = [fiscal_month_from_iso_week(iw) for iw in iso_weeks]\n",
    "   \n",
    "    df = pd.DataFrame({'date_t':dates,\n",
    "    'fiscal_month': fiscal_months})\n",
    " \n",
    "    df_month = (df.groupby('fiscal_month')['date_t'].agg(lambda x: min(x)).reset_index().rename(columns={'date_t':'start_month'}))\n",
    "    df_month['end_month'] =  df.groupby('fiscal_month')['date_t'].agg(lambda x: max(x))\n",
    "    df_month['end_month'] = pd.to_datetime(df_month['end_month']) + pd.DateOffset(days=6)\n",
    "    return df_month\n",
    "   \n",
    "from datetime import date, datetime\n",
    " \n",
    "def date_from_calweek(calweek : int):\n",
    "    iso_year = calweek //100\n",
    "    iso_weeknumber = calweek % 100\n",
    "    iso_weekday = 1\n",
    "    return datetime.strptime(\n",
    "    '{:04d} {:02d} {:d}'.format(iso_year, iso_weeknumber, iso_weekday), '%G %V %u').date()\n",
    "   \n",
    " \n",
    "def fiscal_month(iso_week:int):\n",
    "    year = int(str(iso_week)[:4])\n",
    "    week =  int(str(iso_week)[:-2])\n",
    "    month = month_455_from_iso_week(week)\n",
    "    return int( year *1000 + month)\n",
    " \n",
    "def generate_all_relevant_fiscal_months(current_fiscal_month, n_horizon:int):\n",
    "    current_year = current_fiscal_month // 1000\n",
    "    n_years = n_horizon //12\n",
    "    all_year_required = [current_year + y  for y in range(n_years + 2)]\n",
    "    df_months = pd.concat([generate_fiscal_calendar(y)\n",
    "     for y in all_year_required], ignore_index=True)\n",
    " \n",
    "    ix_current_month =  df_months.query(f'fiscal_month=={current_fiscal_month}').index[0]\n",
    "    result = df_months.iloc[ix_current_month:ix_current_month + n_horizon]\n",
    "    assert len(result) > 0, 'results found'\n",
    "    return result.reset_index(drop=True)\n",
    " \n",
    " \n",
    "def seasonal_naive(row):\n",
    "    return row['lag_12']\n",
    " \n",
    "def create_pipeline_for_model_rf():\n",
    "    # List of all categorical columns\n",
    "    categorical_columns = ['month_of_year', 'ATC_2_ID', 'REGION','MATERIAL_CODE']\n",
    "    # Specify categories for OrdinalEncoder\n",
    "    categories = [\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],  # for 'month_of_year'\n",
    "        'auto',  # for 'ATC_2_ID'\n",
    "        'auto',   # for 'Region'\n",
    "        'auto'\n",
    "    ]\n",
    "    # Create an OrdinalEncoder instance\n",
    "    ordinal_encoder = OrdinalEncoder(\n",
    "        categories=categories,\n",
    "        handle_unknown='use_encoded_value',\n",
    "        unknown_value=-1\n",
    "    )\n",
    "    # Initialize the RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(n_jobs=6, n_estimators=100)\n",
    "    # Create a pipeline with ColumnTransformer and RandomForestRegressor\n",
    "    model = Pipeline([\n",
    "        ('col_trans', ColumnTransformer(\n",
    "            transformers=[('categorical', ordinal_encoder, categorical_columns)],\n",
    "            remainder=\"passthrough\")),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('rf', rf_model)\n",
    "    ])\n",
    "    return model\n",
    " \n",
    "\n",
    "\n",
    " \n",
    "def create_pipeline_for_model_rf():\n",
    "    categorical_columns=['month_of_year']\n",
    "    categories=[[1,2,3,4,5,6,7,8,9,10,11,12]]\n",
    "    ordinal_encoder = OrdinalEncoder(categories=categories,\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1)\n",
    "    rf_model = RandomForestRegressor(n_jobs=6,n_estimators=200,max_depth=4,min_samples_split=4)#,n_estimators=120,max_depth=4,min_samples_split=4,max_features='log2')\n",
    "    model = Pipeline([\n",
    "       ( 'col_trans', ColumnTransformer(transformers=[('categorical',ordinal_encoder,categorical_columns)], remainder=\"passthrough\")),\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('rf',rf_model)\n",
    "    ])\n",
    " \n",
    "    return model\n",
    " \n",
    " \n",
    " \n",
    "def create_pipeline_for_model_xgb():\n",
    "    categorical_columns=['month_of_year']\n",
    "    categories=[[1,2,3,4,5,6,7,8,9,10,11,12]]\n",
    "    ordinal_encoder = OrdinalEncoder(categories=categories,\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1)\n",
    "    xgboost = xgb.XGBRegressor(n_estimators=100,learning_rate=0.01,max_depth=2,booster='gbtree',device='cuda',tree_method='hist')\n",
    "    model = Pipeline([\n",
    "       ( 'col_trans', ColumnTransformer(transformers=[('categorical',ordinal_encoder,categorical_columns)], remainder=\"passthrough\")),\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('xgb',xgboost)\n",
    "    ])\n",
    " \n",
    "    return model\n",
    " \n",
    "def create_pipeline_for_model_lgbm():\n",
    "    categorical_columns = ['month_of_year']\n",
    "    categories = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]\n",
    "    ordinal_encoder = OrdinalEncoder(\n",
    "        categories=categories,\n",
    "        handle_unknown='use_encoded_value',\n",
    "        unknown_value=-1\n",
    "    )\n",
    "    lightgbm = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=2,\n",
    "        boosting_type='goss',\n",
    "        num_leaves=30\n",
    "       \n",
    "    )\n",
    "    model = Pipeline([\n",
    "        ('col_trans', ColumnTransformer(\n",
    "            transformers=[('categorical', ordinal_encoder, categorical_columns)],\n",
    "            remainder='passthrough'\n",
    "        )),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lgbm', lightgbm)\n",
    "    ])\n",
    "    return model\n",
    " \n",
    " \n",
    "def create_features_and_labels_per_horizon(df_ts_key,horizons=None, target='SALES_VALUE_USD'):\n",
    "    #This method considers the target and create multiple features and returns a dataframe with features at multiple horizons\n",
    "    df_out = df_ts_key.copy()\n",
    " \n",
    "    lag_window_size= 12\n",
    "    long_window_size = 24\n",
    " \n",
    "    if horizons is None:\n",
    "        horizons = [0,1,2,3]\n",
    "    df_out['month'] = df_out.month.astype('int64')\n",
    "    df_out['year'] =  df_out.year.astype('int64')\n",
    "    df_out['quarter'] =  (df_out['month'] -1) //3 + 1\n",
    "    df_out['quarter'] = df_out.quarter.astype('int64')\n",
    "    df_out['moving_avg_last_quarter'] = df_out[target].rolling(3).mean(3)\n",
    " \n",
    "    for lag in range(1,13):\n",
    "        df_out[f'lag_{lag}_at_submission_month'] = df_out[target].shift(lag)\n",
    "        df_out[f'lag_{lag}_last_year'] = df_out[target].shift( lag * 12)\n",
    " \n",
    "    for lag in range(1, long_window_size + 1):\n",
    "        df_out[f'lag_{lag}'] = df_out[target].shift(lag)\n",
    "   \n",
    "    for lag in range(1 , long_window_size + 1):\n",
    "        df_out[f'lag_{lag}_moving_avg'] = df_out[f'lag_{lag}'].rolling(window=long_window_size).mean()\n",
    "   \n",
    "    for lag in range(1, 25):\n",
    "        df_out[f'moving_avg_{lag}_months'] = df_out[target].rolling(window = lag * 12).mean()\n",
    "   \n",
    "    for span in [5,10,20]:\n",
    "        df_out[f'ema_{span}'] = df_out[target].ewm(span=span, adjust=False).mean()\n",
    "   \n",
    "    df_out['YoY_growth'] = df_out[target].shift(12)\n",
    "    df_out['Yoy_percentage_growth'] = (df_out[target] / df_out[target].shift(12) - 1) * 100\n",
    " \n",
    "    df_out['quarter_growth'] = df_out[target].shift(3)\n",
    "    df_out['QoQ_percentage_growth'] = (df_out[target] / df_out[target].shift(3) - 1) * 100\n",
    " \n",
    "    df_out['sin_month'] = np.sin(2 * np.pi * (df_out['month'] -1) /3 )\n",
    "    df_out['cos_month'] = np.cos(2 * np.pi * (df_out['month'] -1) /3 )\n",
    " \n",
    "    df_out['diff_at_submission_month'] = df_out[target].shift(1) - df_out[target].shift(2)\n",
    "    df_out['ema_growth'] = df_out['ema_20'].diff()\n",
    " \n",
    "    dfs_per_h=[]\n",
    " \n",
    "    for h in horizons:\n",
    "        df_h = df_out.copy()\n",
    "        df_h['submission_month'] = df_out['fiscal_month']\n",
    "        df_h['fiscal_month'] = df_out['fiscal_month'].shift(-h)\n",
    "        df_h['horizon'] = h\n",
    "        df_h['lag_12'] = df_out[target].shift(12 - h)\n",
    "        df_h['month_of_year'] = [fiscal_month_of_year(x) for x in df_out['fiscal_month'].shift(-h)]\n",
    "        df_h['y'] = df_out[target].shift(-h)\n",
    " \n",
    "        dfs_per_h.append(df_h)\n",
    " \n",
    "    return pd.concat(dfs_per_h, ignore_index=True)\n",
    " \n",
    "   \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "def create_stacking_pipeline_rf_xgb():\n",
    "    rf_pipeline = create_pipeline_for_model_rf()\n",
    "    xgb_pipeline = create_pipeline_for_model_xgb()\n",
    " \n",
    "    estimators = [\n",
    "        ('rf', rf_pipeline),\n",
    "        ('xgb', xgb_pipeline)\n",
    "    ]\n",
    " \n",
    "   \n",
    " \n",
    "    stacking_regressor = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=DecisionTreeRegressor(),\n",
    "        passthrough=True  \n",
    "    )\n",
    " \n",
    "    return stacking_regressor\n",
    " \n",
    "def mi_feature_selection(df_in: pd.DataFrame, features: list, sales_col='clipped_orders', threshold=1.15) -> list:\n",
    "    \"\"\"\n",
    "    Perform mutual information feature selection with improved error handling and data preprocessing.\n",
    "    Parameters:\n",
    "    - df_in: Input DataFrame.\n",
    "    - features: List of feature columns to evaluate.\n",
    "    - sales_col: The target column for sales data.\n",
    "    - threshold: Minimum mutual information score for feature selection.\n",
    "    Returns:\n",
    "    - List of selected features with mutual information score greater than the threshold.\n",
    "    \"\"\"\n",
    "    df_out = df_in.copy()\n",
    "    # Check if all specified features are in the DataFrame\n",
    "    available_features = [f for f in features if f in df_out.columns]\n",
    "    missing_features = set(features) - set(available_features)\n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following features are not in the DataFrame: {missing_features}\")\n",
    "    if sales_col not in df_out.columns:\n",
    "        raise ValueError(f\"Sales column '{sales_col}' not found in the DataFrame.\")\n",
    "    # Ensure sales_col is not in the feature list\n",
    "    if sales_col in available_features:\n",
    "        available_features.remove(sales_col)\n",
    "    # Debug: Print shapes and column counts\n",
    "    #print(f\"Shape of df_out: {df_out.shape}\")\n",
    "    #print(f\"Number of available features: {len(available_features)}\")\n",
    "    #print(f\"Available features: {available_features}\")\n",
    "    # Select only non-null columns for imputation\n",
    "    non_null_features = [f for f in available_features if df_out[f].notna().any()]\n",
    "    #print(f\"Number of non-null features: {len(non_null_features)}\")\n",
    "    #print(f\"Non-null features: {non_null_features}\")\n",
    "    # Debug: Check for NaN and infinite values before processing\n",
    "    nan_count = df_out[non_null_features + [sales_col]].isna().sum()\n",
    "    inf_count = np.isinf(df_out[non_null_features + [sales_col]]).sum()\n",
    "    #print(\"NaN count per column:\", nan_count)\n",
    "    #print(\"Infinite count per column:\", inf_count)\n",
    "    # Replace infinite values with NaN\n",
    "    df_out = df_out.replace([np.inf, -np.inf], np.nan)\n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    try:\n",
    "        imputed_data = imputer.fit_transform(df_out[non_null_features + [sales_col]])\n",
    "        df_imputed = pd.DataFrame(imputed_data,\n",
    "                                  columns=non_null_features + [sales_col],\n",
    "                                  index=df_out.index)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during imputation: {e}\")\n",
    "        raise\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df_imputed[non_null_features]),\n",
    "                             columns=non_null_features,\n",
    "                             index=df_imputed.index)\n",
    "    # Keep the sales column unscaled\n",
    "    df_scaled[sales_col] = df_imputed[sales_col]\n",
    "    # Calculate mutual information scores\n",
    "    mi_scores = mutual_info_regression(df_scaled[non_null_features], df_scaled[sales_col])\n",
    "    # Create a DataFrame from the mutual information scores\n",
    "    mi_score_df = pd.DataFrame({'feature': non_null_features, 'score': mi_scores})\n",
    "    mi_score_df = mi_score_df.sort_values(by='score', ascending=False)\n",
    "    #print(\"Mutual Information Scores:\")\n",
    "    #print(mi_score_df)\n",
    "    # Select features with a score greater than the threshold\n",
    "    sel_feature_list = mi_score_df[mi_score_df['score'] > threshold]['feature'].tolist()\n",
    "    print(f\"\\nSelected {len(sel_feature_list)} features with MI score > {threshold}\")\n",
    "    print(sel_feature_list)\n",
    "    return sel_feature_list\n",
    " \n",
    "def forecast_at_submission_month(submission_month:int , df_ts_key:pd.DataFrame, horizon=None):\n",
    "    if horizon is None:\n",
    "        horizons = [0,1,2,3]\n",
    "    training_data = create_features_and_labels_per_horizon(df_ts_key.query(f'fiscal_month <{submission_month}'),horizons=horizons)\n",
    "    inference_data = create_features_and_labels_per_horizon(df_ts_key.query(f'fiscal_month=={submission_month}'),horizons=horizons)\n",
    "    forecasts=[]\n",
    "    for h in horizons:\n",
    "        training_data_h = training_data.query(f'horizon=={h}').dropna(subset=['y'])\n",
    "        inference_data_h = inference_data.query(f'horizon=={h}')\n",
    " \n",
    "        if training_data_h.empty:\n",
    "            print(training_data_h)\n",
    "            continue\n",
    "           \n",
    "       \n",
    "        if inference_data_h.empty:\n",
    "            print(inference_data_h)\n",
    "            continue\n",
    "        training_data_h.replace([np.inf , -np.inf], np.nan,inplace=True)\n",
    "        training_data_h.fillna(0,inplace=True)\n",
    " \n",
    "        inference_data_h.replace([np.inf , -np.inf], np.nan,inplace=True)\n",
    "        inference_data_h.fillna(0,inplace=True)\n",
    " \n",
    "        features=['moving_avg_last_quarter',\n",
    "         'lag_1_at_submission_month',\n",
    "         'lag_2_at_submission_month',\n",
    "         'lag_3_at_submission_month',\n",
    "          'lag_4_at_submission_month',\n",
    "           'lag_5_at_submission_month',\n",
    "          'lag_6_at_submission_month',\n",
    "          'lag_7_at_submission_month',\n",
    "          'lag_8_at_submission_month',\n",
    "            'ema_growth',\n",
    "           'lag_9_at_submission_month',\n",
    "           'lag_10_at_submission_month',\n",
    "            'lag_11_at_submission_month',\n",
    "            'lag_12_at_submission_month',\n",
    "             'lag_12_last_year',\n",
    "            'lag_11_last_year',\n",
    "            'lag_10_last_year',\n",
    "            'lag_9_last_year',\n",
    "            'lag_8_last_year',\n",
    "            'lag_7_last_year',\n",
    "            'lag_6_last_year',\n",
    "            'lag_5_last_year',\n",
    "            'lag_4_last_year',\n",
    "            'lag_3_last_year',\n",
    "            'lag_2_last_year',\n",
    "            'lag_1_last_year',\n",
    "            'lag_1',\n",
    "            'lag_2',\n",
    "            'lag_3',\n",
    "            'lag_4',\n",
    "            'lag_5',\n",
    "            'lag_6',\n",
    "           'lag_7',\n",
    "            'lag_8',\n",
    "            'lag_9',\n",
    "            'lag_10',\n",
    "            'lag_11',\n",
    "            'lag_12',\n",
    "            'year',\n",
    "            'month',\n",
    "            'lag_1_moving_avg',\n",
    "            'lag_2_moving_avg',\n",
    "            'lag_3_moving_avg',\n",
    "            'lag_4_moving_avg',\n",
    "            'lag_5_moving_avg',\n",
    "            'lag_6_moving_avg',\n",
    "            'lag_7_moving_avg',\n",
    "           'lag_8_moving_avg',\n",
    "            'lag_9_moving_avg',\n",
    "            'lag_10_moving_avg',\n",
    "            'lag_11_moving_avg',\n",
    "            'lag_12_moving_avg',\n",
    "            'ema_5',\n",
    "            'ema_10',\n",
    "             'ema_20',\n",
    "             'QoQ_percentage_growth',\n",
    "             'sin_month',\n",
    "            'cos_month',\n",
    "            'YoY_growth',\n",
    "             'Yoy_percentage_growth',\n",
    "            'diff_at_submission_month',\n",
    "            'moving_avg_1_months',\n",
    "            'moving_avg_2_months',\n",
    "            'moving_avg_3_months',\n",
    "            'moving_avg_4_months',\n",
    "            'moving_avg_5_months',\n",
    "            'moving_avg_6_months',\n",
    "            'moving_avg_7_months',\n",
    "            'moving_avg_8_months',\n",
    "            'moving_avg_9_months',\n",
    "            'moving_avg_10_months',\n",
    "            'moving_avg_11_months',\n",
    "           'moving_avg_12_months',\n",
    "            'moving_avg_13_months',\n",
    "            'moving_avg_14_months',\n",
    "            'moving_avg_15_months',\n",
    "            'moving_avg_16_months',\n",
    "            'moving_avg_17_months',\n",
    "            'moving_avg_18_months',\n",
    "            'moving_avg_19_months',\n",
    "            'moving_avg_21_months',\n",
    "            'moving_avg_22_months',\n",
    "            'moving_avg_23_months',\n",
    "            'moving_avg_24_months',\n",
    "       \n",
    "        'LOST_SALES_VALUE_USD','UNIT_PRICE','TOTAL_FOC_SALES_QTY']\n",
    " \n",
    "        features=mi_feature_selection(df_ts_key,features,'SALES_VALUE_USD')\n",
    "        features.append('month_of_year')\n",
    "        model1 = create_pipeline_for_model_lgbm().fit(training_data_h[features],training_data_h['y'])\n",
    "        forecast_quantity = model1.predict(inference_data_h[features])[0]\n",
    "        forecasts.append(forecast_quantity)\n",
    "   \n",
    " \n",
    "    forecasted = pd.DataFrame()\n",
    "    forecasted['forecast_sales'] = forecasts\n",
    "    forecasted['submission_month'] = submission_month\n",
    "    forecasted['horizon'] = horizons\n",
    " \n",
    "    for col in ['ts_key','COUNTRY_CODE','MATERIAL_GRP4_CODE','MATERIAL_CODE','ATC_2_ID']:\n",
    "        forecasted[col] = df_ts_key[col].iloc[0]\n",
    " \n",
    "    future_months = generate_all_relevant_fiscal_months(submission_month, len(horizons))\n",
    " \n",
    "    forecasted = pd.concat([forecasted, future_months],axis=1)\n",
    " \n",
    "    return forecasted\n",
    "       \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "df_in['fiscal_month'] = df_in.fiscal_month.astype('int64')\n",
    " \n",
    "def replace_outliers(outliers: pd.Series, df_in: pd.DataFrame, input_col, output_col,submission_month):\n",
    "    # Make a copy of the input DataFrame\n",
    "    df_out = df_in.copy()\n",
    "    # Initialize a new column to hold the clean data\n",
    "    df_out['clipped_orders'] = df_out['SALES_VALUE_USD'].copy()\n",
    "    # Filter past sales data based on submission month\n",
    "    past_sales = df_out.query(f'fiscal_month < {submission_month}')['SALES_VALUE_USD']\n",
    "    # Compute the mean value of past sales\n",
    "    mean_value = past_sales.mean()\n",
    "    # Get the indexes of outliers\n",
    "    indexes = outliers.index[outliers].tolist()\n",
    "    # Replace outlier values with the computed mean value\n",
    "    df_out.iloc[indexes, df_out.columns.get_loc('clipped_orders')] = round(mean_value, 2)\n",
    "    return df_out\n",
    " \n",
    " \n",
    " \n",
    "def iqr_outlier_treat( df_in: pd.DataFrame,submission_month, input_col='SALES_VALUE_USD', output_col='clipped_orders') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "     This method uses IQR  to identify outliers, returns a dataframe\n",
    "    \"\"\"\n",
    "    submission_month=submission_month\n",
    "    full_sales = df_in['SALES_VALUE_USD'].copy().reset_index(drop=True)\n",
    "    past_sales = df_in.query(f'fiscal_month<{submission_month}')['SALES_VALUE_USD'].reset_index(drop=True)\n",
    "    q1 = past_sales.quantile(q=0.25)\n",
    "    q3 = past_sales.quantile(q=0.75)\n",
    "    iqr = q3 - q1\n",
    "    low_limit = q1 - 1.5 * iqr\n",
    "    upper_limit = q3 + 1.5 * iqr\n",
    "    outliers = (full_sales < low_limit) | (full_sales > upper_limit)\n",
    "    df_out = replace_outliers(outliers, df_in, input_col, output_col,submission_month)\n",
    "    return df_out\n",
    " \n",
    " \n",
    " \n",
    "res= []\n",
    "submission_month = 2023012\n",
    "for code,data in df_in.groupby('ts_key'):\n",
    "    print(code)\n",
    "    if len(data.query(f'fiscal_month < {submission_month}')) < 30:\n",
    "         continue\n",
    "    #data=iqr_outlier_treat(data,submission_month)\n",
    "    data_per_horizon = create_features_and_labels_per_horizon(data)\n",
    "    forecast_partial  = forecast_at_submission_month(submission_month,data_per_horizon)\n",
    "    res.append(forecast_partial)\n",
    " \n",
    "df_forecasts = pd.concat(res,axis=0)\n",
    "df_forecasts.drop(['start_month','end_month'],axis=1,inplace=True)\n",
    " \n",
    " \n",
    "def evaluate_forecast(experiment_name: str, actuals_df:pd.DataFrame, forecasts_df:pd.DataFrame,mape_group_by=None, save_results=False):\n",
    "    rolling_key=['ts_key','fiscal_month']\n",
    "    monthly_mape_df = forecasts_df.merge(actuals_df,on=rolling_key,how='left', suffixes=('_x',None))\n",
    "    #monthly_mape_df = monthly_mape_df.query('SALES_VALUE_USD !=0')\n",
    "   \n",
    "    monthly_mape_df['abs_error'] = np.abs(monthly_mape_df['SALES_VALUE_USD'] - monthly_mape_df['forecast_sales'])\n",
    "    #monthly_mape_df['unit_price'] = monthly_mape_df['SALES_VALUE_USD'] / monthly_mape_df['NET_SALES_QTY']\n",
    "    #monthly_mape_df['forecasted_sales_quantity'] = monthly_mape_df['unit_price'] * monthly_mape_df['forecast_quantity']\n",
    "    print(monthly_mape_df.head(3))\n",
    " \n",
    " \n",
    "    #monthly_mape_df.to_csv('rf_thailand_country_light_gbm_full_data_1.csv')\n",
    " \n",
    "    if isinstance(monthly_mape_df, (pd.DataFrame,pd.Series)):\n",
    "        if mape_group_by:\n",
    "            result = monthly_mape_df.groupby('fiscal_month')['abs_error'].sum() / monthly_mape_df.groupby('fiscal_month')['SALES_VALUE_USD'].sum() * 100\n",
    "        else:\n",
    "            result = (np.sum(monthly_mape_df['abs_error']) / np.sum(monthly_mape_df['SALES_VALUE_USD']) *100).round(2)\n",
    "       \n",
    "        print(f\"Final MAPE: {result}\")        \n",
    "        return result\n",
    " \n",
    "    else:\n",
    "        raise TypeError(\"monthly_mape_df has to be a dataframe\")\n",
    " \n",
    "    if save_results:\n",
    "        monthly_mape_df.to_csv(f'{experiment_name}_forecasts.csv')\n",
    " \n",
    "    return result,monthly_mape_df\n",
    " \n",
    " \n",
    "evaluate_forecast('Malaysia_matgrp',df_in,df_forecasts,['fiscal_month','horizon'] , save_results=False)\n",
    " \n",
    "def evaluate_forecast(experiment_name: str, actuals_df:pd.DataFrame, forecasts_df:pd.DataFrame,mape_group_by=None, save_results=False):\n",
    "    rolling_key=['ts_key','fiscal_month']\n",
    "    monthly_mape_df = forecasts_df.merge(actuals_df,on=rolling_key,how='left', suffixes=('_x',None))\n",
    "    #monthly_mape_df = monthly_mape_df.query('SALES_VALUE_USD !=0')\n",
    "   \n",
    "    monthly_mape_df['abs_error'] = np.abs(monthly_mape_df['SALES_VALUE_USD'] - monthly_mape_df['forecast_sales'])\n",
    "    #monthly_mape_df['unit_price'] = monthly_mape_df['SALES_VALUE_USD'] / monthly_mape_df['NET_SALES_QTY']\n",
    "    #monthly_mape_df['forecasted_sales_quantity'] = monthly_mape_df['unit_price'] * monthly_mape_df['forecast_quantity']\n",
    "    print(monthly_mape_df.head(3))\n",
    " \n",
    " \n",
    "    #monthly_mape_df.to_csv('rf_thailand_country_light_gbm_full_data_1.csv')\n",
    " \n",
    "    if isinstance(monthly_mape_df, (pd.DataFrame,pd.Series)):\n",
    "        if mape_group_by:\n",
    "            result = monthly_mape_df.groupby('fiscal_month')['abs_error'].sum() / monthly_mape_df.groupby('fiscal_month')['SALES_VALUE_USD'].sum() * 100\n",
    "        else:\n",
    "            result = (np.sum(monthly_mape_df['abs_error']) / np.sum(monthly_mape_df['SALES_VALUE_USD']) *100).round(2)\n",
    "       \n",
    "        print(f\"Final MAPE: {result}\")        \n",
    "        return result\n",
    " \n",
    "    else:\n",
    "        raise TypeError(\"monthly_mape_df has to be a dataframe\")\n",
    " \n",
    "    if save_results:\n",
    "        monthly_mape_df.to_csv(f'{experiment_name}_forecasts.csv')\n",
    " \n",
    "    return result,monthly_mape_df\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de97af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_for_model_lstm_new():\n",
    "    categorical_columns = ['month_of_year']\n",
    "    categories = [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]\n",
    "    ordinal_encoder = OrdinalEncoder(categories=categories, handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    model=Sequential([\n",
    "        LSTM(units=10,return_sequences=True,input_shape=(1,1)),\n",
    "        LSTM(units=5),\n",
    "        Dense(units=1,activation='relu')\n",
    "                        ])\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.001),loss='mean_squared_error')\n",
    "    pipeline=Pipeline([\n",
    "        ('col_trans', ColumnTransformer(transformers=[('categorical', ordinal_encoder, categorical_columns)], remainder=\"passthrough\")),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lstm', model)\n",
    "    ])\n",
    "    return pipeline,model\n",
    "\n",
    "def forecast_at_submission_month_new(submis... by Valli Raja Sekar\n",
    "5:04 pm\n",
    "Valli Raja Sekar\n",
    "def forecast_at_submission_month_new(submission_month:int , df_ts_key:pd.DataFrame, horizon=None):\n",
    "    if horizon is None:\n",
    "        horizons = [0,1,2,3]\n",
    "    training_data = create_features_and_labels_per_horizon(df_ts_key.query(f'fiscal_month <{submission_month}'),horizons=horizons)\n",
    "    inference_data = create_features_and_labels_per_horizon(df_ts_key.query(f'fiscal_month=={submission_month}'),horizons=horizons)\n",
    "    forecasts=[]\n",
    "    for h in horizons:\n",
    "        training_data_h = training_data.query(f'horizon=={h}').dropna(subset=['y'])\n",
    "        inference_data_h = inference_data.query(f'horizon=={h}')\n",
    " \n",
    "        if training_data_h.empty:\n",
    "            print(training_data_h)\n",
    "            continue\n",
    "\n",
    "        if inference_data_h.empty:\n",
    "            print(inference_data_h)\n",
    "            continue\n",
    "        training_data_h.replace([np.inf , -np.inf], np.nan,inplace=True)\n",
    "        training_data_h.fillna(0,inplace=True)\n",
    " \n",
    "        inference_data_h.replace([np.inf , -np.inf], np.nan,inplace=True)\n",
    "        inference_data_h.fillna(0,inplace=True)\n",
    " \n",
    " \n",
    "        #features=mi_feature_selection(df_ts_key,features,'SALES_VALUE_USD')\n",
    "        #features.append('month_of_year')\n",
    "        X_train,y_train=training_data_h[['y']],training_data_h['y']\n",
    "        X_inference=inference_data_h[['y']]\n",
    " \n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        scaler = StandardScaler()\n",
    "        X_train_imputed = imputer.fit_transform(X_train)\n",
    "        X_inference_imputed = imputer.transform(X_inference)\n",
    "        X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "        X_inference_scaled = scaler.transform(X_inference_imputed)\n",
    "        # Reshape for LSTM\n",
    "        X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "        X_inference_reshaped = np.reshape(X_inference_scaled, (X_inference_scaled.shape[0], 1, X_inference_scaled.shape[1]))\n",
    "        pipeline, model = create_pipeline_for_model_lstm_new()\n",
    "        model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "        lstm_forecasts_scaled = model.predict(X_inference_reshaped)\n",
    "        # Inverse transform the scaled forecasts\n",
    "        lstm_forecasts_scaled = scaler.inverse_transform(lstm_forecasts_scaled)\n",
    "        forecast_quantity = lstm_forecasts_scaled.flatten()\n",
    "        forecast_quantity = forecast_quantity[h]\n",
    "        forecasts.append(forecast_quantity)\n",
    "\n",
    " \n",
    "    forecasted = pd.DataFrame()\n",
    "    forecasted['forecast_sales'] = forecasts\n",
    "    forecasted['submission_month'] = submission_month\n",
    " \n",
    "    for col in ['ts_key','REGION','MATERIAL_GRP4_CODE','MATERIAL_CODE']:\n",
    "        forecasted[col] = df_ts_key[col].iloc[0]\n",
    " \n",
    "    future_months = generate_all_relevant_fiscal_months(submission_month, len(horizons))\n",
    " \n",
    "    forecasted = pd.concat([forecasted, future_months],axis=1)\n",
    " \n",
    "    return forecasted\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
